{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch introduction\n",
    "\n",
    "(Py)Torch is a neural network package developed by Facebook, that is extremely simple. We'll use it to implement neural networks\n",
    "\n",
    "# Torch tensor\n",
    "\n",
    "Torch tensors are just like numpy arrays, but with a twist - they can also live on the GPU. Many methods are called similarly to numpy's, but unfortunately there are many API differences. See the documentation for supported functions: http://pytorch.org/docs/master/tensors.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use torch.form_numpy to convert a numpy array. For CPU tensors, it is fast (no memory copy)\n",
    "\n",
    "# Define 4 samples\n",
    "X = torch.from_numpy(np.array([[0,0],\n",
    "                               [0,1],\n",
    "                               [1,0],\n",
    "                               [1,1]], dtype=np.float32))\n",
    "Y = torch.from_numpy(np.array([[0], [1],[1], [0]], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  0\n",
      " 0  1\n",
      " 1  0\n",
      " 1  1\n",
      "[torch.FloatTensor of size 4x2]\n",
      " \n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f280d89f310>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAEICAYAAAC01Po2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYGY4CMwMB4GZAf8PqS/8abLW9koAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f28372b5490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use tensor.numpy to turn a tensor into a numpy array - on CPU, this is fast (no memory copy)\n",
    "scatter(X.numpy()[:,0], X.numpy()[:,1], c=Y.numpy())\n",
    "axis('square')\n",
    "title('The XOR function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hW:  \n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      " hb:  \n",
      "-0.5000 -1.5000\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "H:  \n",
      " 0.0000  0.0000\n",
      " 1.0000  0.0000\n",
      " 1.0000  0.0000\n",
      " 1.0000  1.0000\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define two neurons: an AND neuron, and an OR neuron\n",
    "hW = torch.from_numpy(np.array([[1.0, 1.0],\n",
    "                                [1.0, 1.0]], dtype=np.float32))\n",
    "hb = torch.from_numpy(np.array([[-.5, -1.5]], dtype=np.float32))\n",
    "\n",
    "print('hW: ', hW, 'hb: ', hb)\n",
    "\n",
    "# compute the hidden activation, we multiply by 10. to have the sigmoid look like the step function\n",
    "H = torch.sigmoid(100 * (X.matmul(hW) + hb))\n",
    "\n",
    "print('H: ', H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O:  \n",
      " 1.9287e-22\n",
      " 1.0000e+00\n",
      " 1.0000e+00\n",
      " 1.9287e-22\n",
      "[torch.FloatTensor of size 4x1]\n",
      " Y:  \n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define an output neuron (x1 OR x2) AND NOT (x1 AND x2)\n",
    "\n",
    "oW = torch.from_numpy(np.array([[ 1.0], \n",
    "                                [-1.0]], dtype=np.float32))\n",
    "ob = torch.from_numpy(np.array([[-0.5]], dtype=np.float32))\n",
    "\n",
    "# compute the output, we multiply by 10. to have the sigmoid look like the step function\n",
    "O = torch.sigmoid(100.0 * (H.matmul(oW) + ob))\n",
    "\n",
    "print('O: ', O, 'Y: ', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0072\n",
       " 0.9924\n",
       " 0.9924\n",
       " 0.0072\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's define a function\n",
    "\n",
    "def net(X, hW, hb, oW, ob):\n",
    "    H = torch.sigmoid(10.0 * (X.matmul(hW) + hb))\n",
    "    O = torch.sigmoid(10.0 * (H.matmul(oW) + ob))\n",
    "    return O\n",
    "\n",
    "net(X, hW, hb, oW, ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2178120742a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         raise RuntimeError(\n\u001b[1;32m     83\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "X.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5f7df8c0e9c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Use .cuda() to move variable to the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         raise RuntimeError(\n\u001b[1;32m     83\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# CUDA computations\n",
    "# Use .cuda() to move variable to the GPU\n",
    "\n",
    "net(X.cuda(), hW.cuda(), hb.cuda(), oW.cuda(), ob.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Variable class know about gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0a94ebe2a565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# .data holds the values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"XV: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"XV.data: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         raise RuntimeError(\n\u001b[1;32m     83\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mzr/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "XV = Variable(X.cuda(), requires_grad=False)\n",
    "\n",
    "# .data holds the values\n",
    "print(\"XV: \", XV)\n",
    "print(\"XV.data: \", XV.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out Variable containing:\n",
      " 0.0072\n",
      " 0.9924\n",
      " 0.9924\n",
      " 0.0072\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XV = Variable(X, requires_grad=False)\n",
    "YV = Variable(Y, requires_grad=False)\n",
    "\n",
    "hWV = Variable(hW, requires_grad=True)\n",
    "hbV = Variable(hb, requires_grad=True)\n",
    "oWV = Variable(oW, requires_grad=True)\n",
    "obV = Variable(ob, requires_grad=True)\n",
    "\n",
    "# notice that out will also be a variable\n",
    "out = net(XV, hWV, hbV, oWV, obV)\n",
    "print (\"out\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-04 *\n",
       "  2.1919\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = ((out - YV)**2).sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  \n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "grad:  Variable containing:\n",
      "1.00000e-05 *\n",
      " -7.7096  0.9561\n",
      " -7.7096  0.9561\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .data holds the values\n",
    "print(\"data: \", hWV.data)\n",
    "\n",
    "# .grad holds the accumulated grdient, but \n",
    "print(\"grad: \", hWV.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.00223529339\n",
      "250 0.0633510798216\n",
      "500 0.0106800086796\n",
      "750 0.00550934765488\n",
      "1000 0.0036630590912\n",
      "1250 0.00272816722281\n",
      "1500 0.00216689356603\n",
      "1750 0.00179384509102\n",
      "2000 0.00152852199972\n",
      "2250 0.00133042037487\n",
      "2500 0.00117703096475\n",
      "2750 0.0010548470309\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# Depending on the initialization, this networks fails to train in about 10% cases.\n",
    "# We'll explain why during next lecture.\n",
    "params = [hWV, hbV, oWV, obV]\n",
    "\n",
    "for p in params:\n",
    "    p.data = torch.from_numpy(np.random.randn(*p.size()).astype('float32') * 0.1)\n",
    "\n",
    "for step in range(3000):\n",
    "    for p in params:\n",
    "        # functions ending in _ modify the tensor!\n",
    "        p.grad.data.zero_()\n",
    "    out = net(XV, *params)\n",
    "    loss = ((out - YV)**2).sum()\n",
    "    if (step % 250) == 0:\n",
    "        print(step, loss.data[0])\n",
    "    loss.backward()\n",
    "    for p in params:\n",
    "        p.data -= 1e-2 * p.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hWV,',\n",
      "  Variable containing:\n",
      " 0.4618  0.6597\n",
      " 0.4630  0.6651\n",
      "[torch.FloatTensor of size 2x2]\n",
      "),\n",
      " ('hbV,',\n",
      "  Variable containing:\n",
      "-0.7097 -0.2960\n",
      "[torch.FloatTensor of size 1x2]\n",
      "),\n",
      " ('oWV,',\n",
      "  Variable containing:\n",
      "-1.0539\n",
      " 0.9796\n",
      "[torch.FloatTensor of size 2x1]\n",
      "),\n",
      " ('obV', Variable containing:\n",
      "-0.4525\n",
      "[torch.FloatTensor of size 1x1]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(zip ('hWV, hbV, oWV, obV'.split(), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1:\n",
    "    for p in params:\n",
    "        # functions ending in _ modify the tensor!\n",
    "        p.grad.data.zero_()\n",
    "    out = net(XV, *params)\n",
    "    loss = ((out - YV)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.00059986115\n",
      "250 0.999978721142\n",
      "500 0.999841153622\n",
      "750 0.997927606106\n",
      "1000 0.206064373255\n",
      "1250 0.0362930893898\n",
      "1500 0.0183573663235\n",
      "1750 0.0120765883476\n",
      "2000 0.0089367441833\n",
      "2250 0.00706796068698\n",
      "2500 0.00583356944844\n",
      "2750 0.00495963683352\n"
     ]
    }
   ],
   "source": [
    "# Alt network - this one is easier to train and converges most of the time!\n",
    "params = [hWV, hbV, oWV, obV]\n",
    "\n",
    "def net2(X, hW, hb, oW, ob):\n",
    "    H = torch.tanh(((X * 2.0 - 1.0).matmul(hW) + hb))\n",
    "    O = torch.sigmoid((H.matmul(oW) + ob))\n",
    "    return O\n",
    "\n",
    "for p in params:\n",
    "    p.data = torch.from_numpy(np.random.randn(*p.size()).astype('float32') * 0.1)\n",
    "\n",
    "for step in range(3000):\n",
    "    for p in params:\n",
    "        # functions ending in _ modify the tensor!\n",
    "        p.grad.data.zero_()\n",
    "    out = net2(XV, *params)\n",
    "    loss = ((out - YV)**2).sum()\n",
    "    if (step % 250) == 0:\n",
    "        print(step, loss.data[0])\n",
    "    loss.backward()\n",
    "    for p in params:\n",
    "        p.data -= p.grad.data * 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
